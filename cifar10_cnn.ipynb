{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "from torchvision import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    # convert data to a normalized torch.FloatTensor\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "    ])\n",
    "\n",
    "    # choose the training and test datasets\n",
    "    train_data = datasets.CIFAR10('data', train=True,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "    test_data = datasets.CIFAR10('data', train=False,\n",
    "                                 download=True, transform=transform)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(train_data, testdata):\n",
    "    \n",
    "    # number of subprocesses to use for data loading\n",
    "    num_workers = 0\n",
    "\n",
    "    # how many samples per batch to load\n",
    "    batch_size = 20\n",
    "\n",
    "    # percentage of training set to use as validation\n",
    "    valid_size = 0.2\n",
    "\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(valid_size*num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "\n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    # prepare data loaders (combine dataset and sampler)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                               sampler=train_sampler, num_workers=num_workers)\n",
    "    valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                               sampler=valid_sampler, num_workers=num_workers)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
    "                                              num_workers=num_workers)\n",
    "\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
    "\n",
    "def visualize(loader):\n",
    "    data_iter = iter(loader)\n",
    "    images, labels = data_iter.next()\n",
    "    images = images.numpy()\n",
    "    \n",
    "    # plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(25,4))\n",
    "    \n",
    "    # display 20 images\n",
    "    for idx in range(20):\n",
    "        ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "        imshow(images[idx])\n",
    "        ax.set_title(classes[labels[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN architecture\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Conv Layer 1\n",
    "        self.conv1 = nn.Conv2d(3,16,3,padding=1)\n",
    "        # Conv Layer 2\n",
    "        self.conv2 = nn.Conv2d(16,32,3,padding=1)\n",
    "        # Conv Layer 3\n",
    "        self.conv3 = nn.Conv2d(32,64,3,padding=1)\n",
    "        \n",
    "        # Max Pooling Layer\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        # Linear Layers\n",
    "        self.fc1 = nn.Linear(4*4*64,500)\n",
    "        self.fc2 = nn.Linear(500,10)\n",
    "        \n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Perform convolutions on input with max pool layer and relu activation\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        \n",
    "        # Flatten view of the convolutional layer\n",
    "        x = x.view(-1,4*4*64)\n",
    "        \n",
    "        # Add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Add first linear layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # Add another dropout layer\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Add second linear layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "\n",
    "def train(model, train_loader, valid_loader, criterion, optimizer):\n",
    "    n_epochs = 30\n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    for e in range(1,n_epochs+1):\n",
    "        \n",
    "        # Keep track of train and valid loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        # Train Model #\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "        \n",
    "        # Validate Model #\n",
    "        model.eval()\n",
    "        for data, target in valid_loader:\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "        \n",
    "        # Calculate average loss\n",
    "        train_loss = train_loss/len(train_loader.sampler)\n",
    "        valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "        \n",
    "        # Print the values\n",
    "        print('Epich: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "             e, train_loss, valid_loss))\n",
    "        \n",
    "        # Save model if loss has decreased\n",
    "        \n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss has reduced from {:.6f} to {:.6f}. Saving model ...'.format(\n",
    "            train_loss, valid_loss))\n",
    "            torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "            valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion):\n",
    "    test_loss = 0.0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "\n",
    "    model.eval()\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.numpy())\n",
    "        for i in range(20): # over batch_size\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "    \n",
    "    # average test loss\n",
    "    test_loss = test_loss/len(test_loader.dataset)\n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "    for i in range(10):\n",
    "        if class_total[i] > 0:\n",
    "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "                classes[i], 100 * class_correct[i] / class_total[i],\n",
    "                np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "        else:\n",
    "            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "        100. * np.sum(class_correct) / np.sum(class_total),\n",
    "        np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the image classes\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "train_data, test_data = loadDataSet()\n",
    "\n",
    "train_loader, valid_loader, test_loader = loader(train_data, test_data)\n",
    "\n",
    "visualize(train_loader)\n",
    "\n",
    "model = Net()\n",
    "print(model)\n",
    "\n",
    "# Specify loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "if (glob.glob('model_cifar.pt')):\n",
    "    model.load_state_dict(torch.load('model_cifar.pt'))\n",
    "    test(model, test_loader, criterion)\n",
    "else:\n",
    "    train(model, train_loader, valid_loader, criterion, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
